# 필요한 라이브러리 설치
!pip install pandas langchain langchain-community langchain-chroma sentence-transformers torch
!pip install --upgrade pip
!pip install langchain-huggingface
!pip install pysqlite3-binary
!pip install --upgrade chromadb langchain-community


__import__('pysqlite3')
import sys
sys.modules['sqlite3'] = sys.modules.pop('pysqlite3')

import pandas as pd
from langchain_community.document_loaders import DataFrameLoader
from langchain_text_splitters import RecursiveCharacterTextSplitter
from langchain_community.embeddings import HuggingFaceBgeEmbeddings
from langchain_community.vectorstores import Chroma
from langchain.prompts import PromptTemplate
from langchain_community.chat_models import ChatOllama
from langchain.schema.runnable import RunnablePassthrough
from langchain.schema.output_parser import StrOutputParser
import os
import shutil
from pprint import pprint
import torch

# -----------------------------------------------------------------------------
# 1. 데이터 로드 및 전처리
# -----------------------------------------------------------------------------
# 사용자가 제공한 'titanic.csv' 파일을 불러옵니다.
csv_file_path = './data/titanic.csv'
df = pd.read_csv(csv_file_path)

df['Survived_str'] = df['Survived'].apply(lambda x: '생존' if x == 1 else '사망')
df['combined_info'] = (
    df['Name'] + "은(는) " +
    df['Sex'] + "성 승객으로, 나이는 " + df['Age'].astype(str) + "세입니다. " +
    "탑승 등급은 " + df['Pclass'].astype(str) + "등급이었으며, 최종적으로 " +
    df['Survived_str'] + "했습니다."
)

loader = DataFrameLoader(df, page_content_column='combined_info')
docs = loader.load()

text_splitter = RecursiveCharacterTextSplitter(
    chunk_size=500,
    chunk_overlap=50,
    length_function=len
)
split_docs = text_splitter.split_documents(docs)

print(f"원본 문서 수: {len(docs)}")
print(f"분할된 문서 수: {len(split_docs)}\n")




# -----------------------------------------------------------------------------
# 2. 임베딩 모델 및 벡터 스토어 설정
# -----------------------------------------------------------------------------
# Hugging Face Hub에서 'bge-large-ko' 모델을 자동으로 다운로드하여 사용합니다.
model_name = "BAAI/bge-m3"
# GPU가 사용 가능한지 확인하고 device를 설정합니다.
if torch.cuda.is_available():
    device = 'cuda'
    print("GPU가 감지되었습니다. 임베딩에 GPU를 사용합니다.")
else:
    device = 'cpu'
    print("GPU가 감지되지 않았습니다. 임베딩에 CPU를 사용합니다.")
encode_kwargs = {'normalize_embeddings': True}


print("-------------------1")

embeddings = HuggingFaceBgeEmbeddings(
    model_name=model_name,
    model_kwargs=model_kwargs,
    encode_kwargs=encode_kwargs
)

print("-------------------2")
persist_directory = 'chroma_db'
vectorstore = Chroma.from_documents(
    documents=split_docs,
    embedding=embeddings,
    persist_directory=persist_directory
)

print("-------------------3")
retriever = vectorstore.as_retriever(search_kwargs={"k": 3})

print("-------------------4")
# -----------------------------------------------------------------------------
# 3. LLM 모델 및 RAG Chain 구성
# -----------------------------------------------------------------------------
llm = ChatOllama(model="llama3")

print("-------------------5")
template = """
당신은 사용자의 질문에 답하는 친절한 한국어 챗봇입니다.
주어진 문맥(context)을 사용하여 질문(question)에 답하세요.
만약 문맥에 관련 정보가 없거나 질문에 답할 수 없다면, 모른다고 답변하세요.

문맥: {context}

질문: {question}

답변:
"""
prompt = PromptTemplate.from_template(template)

print("-------------------6")
rag_chain = (
    {"context": retriever, "question": RunnablePassthrough()}
    | prompt
    | llm
    | StrOutputParser()
)

print("-------------------7")
print("챗봇이 준비되었습니다. '종료'를 입력하면 대화가 끝납니다.\n")
# -----------------------------------------------------------------------------
# 4. 챗봇 대화 루프
# -----------------------------------------------------------------------------
while True:
    try:
        user_question = input("질문하세요: ")
        if user_question.lower() == '종료':
            print("대화를 종료합니다.")
            break

        result = rag_chain.invoke(user_question)
        print("챗봇 답변:")
        pprint(result)

    except Exception as e:
        print(f"오류가 발생했습니다: {e}")
        break





